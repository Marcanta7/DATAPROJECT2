{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.metrics import Metrics\n",
    "\n",
    "# B. Apache Beam ML Libraries\n",
    "from apache_beam.ml.inference.base import ModelHandler\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "\n",
    "# C. Python Libraries\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "\n",
    "beam.options.pipeline_options.PipelineOptions.allow_non_parallel_instruction_output = True\n",
    "DataflowRunner.__test__ = False\n",
    "\n",
    "def ParsePubSubMessages(message): \n",
    "    pubsub_message= message.decode('utf-8')\n",
    "\n",
    "    msg = json.loads(pubsub_message)\n",
    "\n",
    "    logging.info(\"New message: %s\", msg)\n",
    "\n",
    "    return msg\n",
    "\n",
    "def run(): \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=('Input arguments for the Dataflow Streaming Pipeline.'))\n",
    "\n",
    "    parser.add_argument(\n",
    "                '--project_id',\n",
    "                required=True,\n",
    "                help='GCP cloud project name, in this case data-project-2425')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--affected_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading affected people. In this case the subscripcion will be: affected-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--volunteer_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading volunteer prople. In this case the subscripcion will be: volunteer-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--output_topic_non_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of non matched messages. In this case it will be: no-matched')\n",
    "        \n",
    "    parser.add_argument(\n",
    "                '--output_topic_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of matched messages. In this case: matched')\n",
    "    \n",
    "    args, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    options = PipelineOptions(pipeline_opts, \n",
    "        save_main_session= True, streaming= True, project= args.project_id)\n",
    "    \n",
    "    with beam.Pipeline(argv= pipeline_opts, options=options) as p:\n",
    "\n",
    "        affected_data=(\n",
    "            p\n",
    "                |\"Read affected data from Pub/Sub\" >> beam.io.ReadFromPubSub(subscription= args.affected_sub)\n",
    "                |\"Parse Json battery messages\" >> beam.Map(ParsePubSubMessages)\n",
    "                |\" Fixed window for Affected data\" >>beam.WindowInto(beam.window.FixedWindows(90))\n",
    "        )\n",
    "\n",
    "        volunteer_data=(\n",
    "            p \n",
    "                |\"Read volunteer data from Pub/Sub\">> beam.io.ReadFromPubSub(subscription=args.volunteer_sub)\n",
    "                |\"Parse Json from Volunteer messages\">> beam.Map(ParsePubSubMessages)\n",
    "                |\"Fixed window for Volunteer data\" >> beam.WindowInto(beam.window.FixedWindows(90))\n",
    "        )\n",
    "\n",
    "        logging.info(affected_data)\n",
    "        logging.info(volunteer_data)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Disable logs from apache_beam.utils.subprocess_server\n",
    "    logging.getLogger(\"apache_beam.utils.subprocess_server\").setLevel(logging.ERROR)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "\n",
    "    # Run Process\n",
    "    run()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Till here i want to prove if the code is correct let's run it on dataflow  run pipeline in GCP: dataflow\n",
    "\n",
    "        python dataflow_pipeline.py \\\n",
    "    --project_id 'data-project-2425' \\\n",
    "    --affected_sub 'projects/data-project-2425/subscriptions/affected-sub' \\\n",
    "    --volunteer_sub 'projects/data-project-2425/subscriptions/volunteer-sub' \\\n",
    "    --output_topic_non_matched 'projects/data-project-2425/topics/no-matched' \\\n",
    "    --output_topic_matched 'projects/data-project-2425/topics/matched' \\\n",
    "    --system_id 'vvercherg' \\\n",
    "    --runner DataflowRunner \\\n",
    "    --job_name 'data-flow-pruebas-1' \\\n",
    "    --region 'europe-west1' \\\n",
    "    --temp_location 'gs://dataflow_bucket_dataproject_2425/tmp' \\\n",
    "    --staging_location 'gs://dataflow_bucket_dataproject_2425/stg' \\\n",
    "    --requirements_file 'requirements.txt'\n",
    "\n",
    "    \n",
    "    de aqui sacamos en claro que lee los mensajes y que le hace la window de 90 segundos\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora vamos a hacer que haga los matches y que los envie a otro pubsub,vamos a ver que nos sale KAKAKAKKA\n",
    "\n",
    "estoy desvariando ya creo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.metrics import Metrics\n",
    "\n",
    "# B. Apache Beam ML Libraries\n",
    "from apache_beam.ml.inference.base import ModelHandler\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "\n",
    "# C. Python Libraries\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "\n",
    "beam.options.pipeline_options.PipelineOptions.allow_non_parallel_instruction_output = True\n",
    "DataflowRunner.__test__ = False\n",
    "\n",
    "def ParsePubSubMessages(message): \n",
    "    pubsub_message= message.decode('utf-8')\n",
    "\n",
    "    msg = json.loads(pubsub_message)\n",
    "\n",
    "    logging.info(\"New message: %s\", msg)\n",
    "\n",
    "    return msg['city'], msg\n",
    "\n",
    "class BusinessLogicDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "\n",
    "        city, grouped_data = element\n",
    "        affected_list = grouped_data.get('affected', [])\n",
    "        volunteer_list = grouped_data.get('volunteer', [])\n",
    "        \n",
    "        matched_data = []\n",
    "        unmatched_data = []\n",
    "        \n",
    "        for affected in affected_list:\n",
    "            matched = False\n",
    "            for volunteer in volunteer_list:\n",
    "                if (affected['necessity'] == volunteer['necessity'] and\n",
    "                    affected['disponibility'] == volunteer['disponibility'] and\n",
    "                    affected['city'] == volunteer['city']\n",
    "                    ):\n",
    "                    matched_data.append({\n",
    "                        \"affected\": affected,\n",
    "                        \"volunteer\": volunteer,\n",
    "                        \"matched_timestamp\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                    })\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                unmatched_data.append(affected)\n",
    "        \n",
    "        for volunteer in volunteer_list:\n",
    "            if all(volunteer != match['volunteer'] for match in matched_data):\n",
    "                unmatched_data.append(volunteer)\n",
    "        \n",
    "        yield beam.pvalue.TaggedOutput(\"matched_data\", matched_data)\n",
    "        yield beam.pvalue.TaggedOutput(\"unmatched_data\", unmatched_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run(): \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=('Input arguments for the Dataflow Streaming Pipeline.'))\n",
    "\n",
    "    parser.add_argument(\n",
    "                '--project_id',\n",
    "                required=True,\n",
    "                help='GCP cloud project name, in this case data-project-2425')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--affected_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading affected people. In this case the subscripcion will be: affected-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--volunteer_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading volunteer prople. In this case the subscripcion will be: volunteer-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--output_topic_non_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of non matched messages. In this case it will be: no-matched')\n",
    "        \n",
    "    parser.add_argument(\n",
    "                '--output_topic_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of matched messages. In this case: matched')\n",
    "    \n",
    "    args, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    options = PipelineOptions(pipeline_opts, \n",
    "        save_main_session= True, streaming= True, project= args.project_id)\n",
    "    \n",
    "    with beam.Pipeline(argv= pipeline_opts, options=options) as p:\n",
    "\n",
    "        affected_data=(\n",
    "            p\n",
    "                |\"Read affected data from Pub/Sub\" >> beam.io.ReadFromPubSub(subscription= args.affected_sub)\n",
    "                |\"Parse Json battery messages\" >> beam.Map(ParsePubSubMessages)\n",
    "                |\" Fixed window for Affected data\" >>beam.WindowInto(beam.window.FixedWindows(90))\n",
    "        )\n",
    "\n",
    "        volunteer_data=(\n",
    "            p \n",
    "                |\"Read volunteer data from Pub/Sub\">> beam.io.ReadFromPubSub(subscription=args.volunteer_sub)\n",
    "                |\"Parse Json from Volunteer messages\">> beam.Map(ParsePubSubMessages)\n",
    "                |\"Fixed window for Volunteer data\" >> beam.WindowInto(beam.window.FixedWindows(90))\n",
    "        )\n",
    "\n",
    "        # co Group by key \n",
    "        grouped_data= (\n",
    "            affected_data, volunteer_data) | \"merge PCollection\" >> beam.CoGroupByKey()\n",
    "\n",
    "        proces_data= (grouped_data\n",
    "            |\"check the matched messages\" >> beam.ParDo(BusinessLogicDoFn()).with_outputs(\"matched_data\", \"unmatched_data\"))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Disable logs from apache_beam.utils.subprocess_server\n",
    "    logging.getLogger(\"apache_beam.utils.subprocess_server\").setLevel(logging.ERROR)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "\n",
    "    # Run Process\n",
    "    run()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Till here i want to prove if the code is correct let's run it on dataflow  run pipeline in GCP: dataflow\n",
    "\n",
    "        python dataflow_pipeline.py \\\n",
    "    --project_id 'data-project-2425' \\\n",
    "    --affected_sub 'projects/data-project-2425/subscriptions/affected-sub' \\\n",
    "    --volunteer_sub 'projects/data-project-2425/subscriptions/volunteer-sub' \\\n",
    "    --output_topic_non_matched 'projects/data-project-2425/topics/no-matched' \\\n",
    "    --output_topic_matched 'projects/data-project-2425/topics/matched' \\\n",
    "    --system_id 'vvercherg' \\\n",
    "    --runner DataflowRunner \\\n",
    "    --job_name 'data-flow-pruebas-1' \\\n",
    "    --region 'europe-west1' \\\n",
    "    --temp_location 'gs://dataflow_bucket_dataproject_2425/tmp' \\\n",
    "    --staging_location 'gs://dataflow_bucket_dataproject_2425/stg' \\\n",
    "    --requirements_file 'requirements.txt'\n",
    "\n",
    "    \n",
    "    de aqui sacamos en claro que lee los mensajes y que le hace la window de 90 segundos\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Till here i want to prove if the code is correct let's run it on dataflow  run pipeline in GCP: dataflow\n",
    "\n",
    "        python dataflow_pipeline.py \\\n",
    "    --project_id 'data-project-2425' \\\n",
    "    --affected_sub 'projects/data-project-2425/subscriptions/affected-sub' \\\n",
    "    --volunteer_sub 'projects/data-project-2425/subscriptions/volunteer-sub' \\\n",
    "    --output_topic_non_matched 'projects/data-project-2425/topics/no-matched' \\\n",
    "    --output_topic_matched 'projects/data-project-2425/topics/matched' \\\n",
    "    --system_id 'vvercherg' \\\n",
    "    --runner DataflowRunner \\\n",
    "    --job_name 'data-flow-pruebas-7' \\\n",
    "    --region 'europe-west1' \\\n",
    "    --temp_location 'gs://dataflow_bucket_dataproject_2425/tmp' \\\n",
    "    --staging_location 'gs://dataflow_bucket_dataproject_2425/stg' \\\n",
    "    --requirements_file 'requirements.txt'\n",
    "\n",
    "    \n",
    "    de aqui sacamos en claro que lee los mensajes y que le hace la window de 90 segundos\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "script que funciona bien 100% le quedan algunas cosiitas pero este no tocar, el de abajo claro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.metrics import Metrics\n",
    "\n",
    "# B. Apache Beam ML Libraries\n",
    "from apache_beam.ml.inference.base import ModelHandler\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "\n",
    "# C. Python Libraries\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "def ParsePubSubMessages(message): \n",
    "    pubsub_message= message.decode('utf-8')\n",
    "\n",
    "    msg = json.loads(pubsub_message)\n",
    "\n",
    "    #logging.info(\"New message: %s\", msg)\n",
    "\n",
    "    return msg\n",
    "\n",
    "def key_by_match_fields(record):\n",
    "    \"\"\"\n",
    "    Returns a tuple: (\n",
    "        (city, necessity, disponibility),\n",
    "        record\n",
    "    )\n",
    "    \"\"\"\n",
    "    return (\n",
    "        (record[\"city\"], record[\"necessity\"], record[\"disponibility\"]), \n",
    "        record\n",
    "    )\n",
    "\n",
    "def produce_matches(element):\n",
    "    \"\"\"\n",
    "    Receives something like:\n",
    "        element = ( key, { 'affected': [...], 'volunteer': [...] } )\n",
    "    and we produce:\n",
    "        - All matched (afectado, voluntario) pairs\n",
    "        - All affected with NO volunteer\n",
    "        - All volunteer with NO affected\n",
    "    \"\"\"\n",
    "    key, grouped = element\n",
    "    afectados = grouped['affected']\n",
    "    voluntarios = grouped['volunteer']\n",
    "\n",
    "\n",
    "    for afectado in afectados:\n",
    "        found_any = False\n",
    "        for voluntario in voluntarios:\n",
    "            found_any = True\n",
    "            yield beam.pvalue.TaggedOutput(\n",
    "                'matched',\n",
    "                {\n",
    "                    'afectado': afectado,\n",
    "                    'voluntario': voluntario\n",
    "                }\n",
    "            )\n",
    "        if not found_any:\n",
    "            # This afectado had zero volunteers\n",
    "            yield beam.pvalue.TaggedOutput(\n",
    "                'non_matched_affected',\n",
    "                afectado\n",
    "            )\n",
    "\n",
    "    if not afectados:\n",
    "\n",
    "        for voluntario in voluntarios:\n",
    "            yield beam.pvalue.TaggedOutput('non_matched_volunteer', voluntario)\n",
    "\n",
    "\n",
    "def run():\n",
    "    parser = argparse.ArgumentParser(description=('Input arguments for the Dataflow Streaming Pipeline.'))\n",
    "\n",
    "    parser.add_argument(\n",
    "                '--project_id',\n",
    "                required=True,\n",
    "                help='GCP cloud project name, in this case data-project-2425')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--affected_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading affected people. In this case the subscripcion will be: affected-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--volunteer_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading volunteer prople. In this case the subscripcion will be: volunteer-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--output_topic_non_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of non matched messages. In this case it will be: no-matched')\n",
    "        \n",
    "    parser.add_argument(\n",
    "                '--output_topic_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of matched messages. In this case: matched')\n",
    "    \n",
    "    args, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    options = PipelineOptions(pipeline_opts, \n",
    "        save_main_session= True, streaming= True, project= args.project_id)\n",
    "    \n",
    "    with beam.Pipeline(options=options) as p:\n",
    "        affected_data = (\n",
    "            p\n",
    "            | \"Read affected data from Pub/Sub\" >> beam.io.ReadFromPubSub(subscription=args.affected_sub)\n",
    "            | \"Parse Affected\" >> beam.Map(ParsePubSubMessages)\n",
    "            | \"Window Affected\" >> beam.WindowInto(beam.window.FixedWindows(90))\n",
    "            | \"Key Affected\" >> beam.Map(key_by_match_fields)\n",
    "        )\n",
    "\n",
    "        volunteer_data = (\n",
    "            p\n",
    "            | \"Read volunteer data from Pub/Sub\" >> beam.io.ReadFromPubSub(subscription=args.volunteer_sub)\n",
    "            | \"Parse Volunteer\" >> beam.Map(ParsePubSubMessages)\n",
    "            | \"Window Volunteer\" >> beam.WindowInto(beam.window.FixedWindows(90))\n",
    "            | \"Key Volunteer\" >> beam.Map(key_by_match_fields)\n",
    "        )\n",
    "\n",
    "        # Join them by CoGroupByKey\n",
    "        grouped = (\n",
    "            {\n",
    "                'affected': affected_data,\n",
    "                'volunteer': volunteer_data\n",
    "            }\n",
    "            | \"CoGroupByKey\" >> beam.CoGroupByKey()\n",
    "        )\n",
    "\n",
    "        # Produce separate PCollections for matched, non_matched_affected, non_matched_volunteer\n",
    "        results = (\n",
    "            grouped\n",
    "            | \"Match DoFn\" >> beam.ParDo(produce_matches)\n",
    "              .with_outputs('matched', 'non_matched_affected', 'non_matched_volunteer')\n",
    "        )\n",
    "\n",
    "        matched_pcoll = results['matched']\n",
    "        unmatched_affected_pcoll = results['non_matched_affected']\n",
    "        unmatched_volunteer_pcoll = results['non_matched_volunteer']\n",
    "\n",
    "        # Now you can write them separately to Pub/Sub, or handle them differently:\n",
    "        # 1) matched\n",
    "        (matched_pcoll\n",
    "         | 'codear matches'>>beam.Map(lambda x: json.dumps(x).encode(\"utf-8\"))\n",
    "         | 'write matched data' >> beam.io.WriteToPubSub(topic=args.output_topic_matched)\n",
    "        )\n",
    "\n",
    "        # 2) unmatched affected\n",
    "        (unmatched_affected_pcoll\n",
    "         | 'codear no matches aff'>>beam.Map(lambda x: json.dumps(x).encode(\"utf-8\"))\n",
    "         | 'write affected'>> beam.io.WriteToPubSub(topic=args.output_topic_non_matched)\n",
    "        )\n",
    "\n",
    "        # 3) unmatched volunteers\n",
    "        # Possibly you want a separate Pub/Sub topic for them\n",
    "        # or just the same topic—adjust as needed.\n",
    "        (unmatched_volunteer_pcoll\n",
    "         | 'codear no matches vol'>>beam.Map(lambda x: json.dumps(x).encode(\"utf-8\"))\n",
    "         | 'write volunteers'>>beam.io.WriteToPubSub(topic=args.output_topic_non_matched)\n",
    "        )\n",
    "\n",
    "       \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Disable logs from apache_beam.utils.subprocess_server\n",
    "    logging.getLogger(\"apache_beam.utils.subprocess_server\").setLevel(logging.ERROR)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "\n",
    "    # Run Process\n",
    "    run()\n",
    "\n",
    "    '''\n",
    "\n",
    "Till here i want to prove if the code is correct let's run it on dataflow  run pipeline in GCP: dataflow\n",
    "\n",
    "        python new_dataflow_pipeline.py \\\n",
    "    --project_id 'data-project-2425' \\\n",
    "    --affected_sub 'projects/data-project-2425/subscriptions/affected-sub' \\\n",
    "    --volunteer_sub 'projects/data-project-2425/subscriptions/volunteer-sub' \\\n",
    "    --output_topic_non_matched 'projects/data-project-2425/topics/no-matched' \\\n",
    "    --output_topic_matched 'projects/data-project-2425/topics/matched' \\\n",
    "    --system_id 'vvercherg' \\\n",
    "    --runner DataflowRunner \\\n",
    "    --job_name 'data-flow-pruebas-999' \\\n",
    "    --region 'europe-west1' \\\n",
    "    --temp_location 'gs://dataflow_bucket_dataproject_2425/tmp' \\\n",
    "    --staging_location 'gs://dataflow_bucket_dataproject_2425/stg' \\\n",
    "    --requirements_file 'requirements.txt'\n",
    "\n",
    "    \n",
    "correrlo de forma local\n",
    "\n",
    "    python new_dataflow_pipeline.py \\\n",
    "    --project_id 'data-project-2425' \\\n",
    "    --affected_sub 'projects/data-project-2425/subscriptions/affected-sub' \\\n",
    "    --volunteer_sub 'projects/data-project-2425/subscriptions/volunteer-sub' \\\n",
    "    --output_topic_non_matched 'projects/data-project-2425/topics/no-matched' \\\n",
    "    --output_topic_matched 'projects/data-project-2425/topics/matched' \n",
    "    \n",
    "\n",
    "\n",
    "    topics :\n",
    "\n",
    "    affected \n",
    "    volunteer\n",
    "    matched\n",
    "    no-matched \n",
    "    \n",
    "    de aqui sacamos en claro que lee los mensajes y que le hace la window de 90 segundos\n",
    "\n",
    "\n",
    "    NO TOCAR ESTE SCRIPT \n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
