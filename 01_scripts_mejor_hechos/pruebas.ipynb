{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.metrics import Metrics\n",
    "\n",
    "# B. Apache Beam ML Libraries\n",
    "from apache_beam.ml.inference.base import ModelHandler\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "\n",
    "# C. Python Libraries\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "\n",
    "beam.options.pipeline_options.PipelineOptions.allow_non_parallel_instruction_output = True\n",
    "DataflowRunner.__test__ = False\n",
    "\n",
    "def ParsePubSubMessages(message): \n",
    "    pubsub_message= message.decode('utf-8')\n",
    "\n",
    "    msg = json.loads(pubsub_message)\n",
    "\n",
    "    logging.info(\"New message: %s\", msg)\n",
    "\n",
    "    return msg\n",
    "\n",
    "def run(): \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=('Input arguments for the Dataflow Streaming Pipeline.'))\n",
    "\n",
    "    parser.add_argument(\n",
    "                '--project_id',\n",
    "                required=True,\n",
    "                help='GCP cloud project name, in this case data-project-2425')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--affected_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading affected people. In this case the subscripcion will be: affected-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--volunteer_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading volunteer prople. In this case the subscripcion will be: volunteer-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--output_topic_non_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of non matched messages. In this case it will be: no-matched')\n",
    "        \n",
    "    parser.add_argument(\n",
    "                '--output_topic_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of matched messages. In this case: matched')\n",
    "    \n",
    "    args, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    options = PipelineOptions(pipeline_opts, \n",
    "        save_main_session= True, streaming= True, project= args.project_id)\n",
    "    \n",
    "    with beam.Pipeline(argv= pipeline_opts, options=options) as p:\n",
    "\n",
    "        affected_data=(\n",
    "            p\n",
    "                |\"Read affected data from Pub/Sub\" >> beam.io.ReadFromPubSub(subscription= args.affected_sub)\n",
    "                |\"Parse Json battery messages\" >> beam.Map(ParsePubSubMessages)\n",
    "                |\" Fixed window for Affected data\" >>beam.WindowInto(beam.window.FixedWindows(90))\n",
    "        )\n",
    "\n",
    "        volunteer_data=(\n",
    "            p \n",
    "                |\"Read volunteer data from Pub/Sub\">> beam.io.ReadFromPubSub(subscription=args.volunteer_sub)\n",
    "                |\"Parse Json from Volunteer messages\">> beam.Map(ParsePubSubMessages)\n",
    "                |\"Fixed window for Volunteer data\" >> beam.WindowInto(beam.window.FixedWindows(90))\n",
    "        )\n",
    "\n",
    "        logging.info(affected_data)\n",
    "        logging.info(volunteer_data)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Disable logs from apache_beam.utils.subprocess_server\n",
    "    logging.getLogger(\"apache_beam.utils.subprocess_server\").setLevel(logging.ERROR)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "\n",
    "    # Run Process\n",
    "    run()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Till here i want to prove if the code is correct let's run it on dataflow  run pipeline in GCP: dataflow\n",
    "\n",
    "        python dataflow_pipeline.py \\\n",
    "    --project_id 'data-project-2425' \\\n",
    "    --affected_sub 'projects/data-project-2425/subscriptions/affected-sub' \\\n",
    "    --volunteer_sub 'projects/data-project-2425/subscriptions/volunteer-sub' \\\n",
    "    --output_topic_non_matched 'projects/data-project-2425/topics/no-matched' \\\n",
    "    --output_topic_matched 'projects/data-project-2425/topics/matched' \\\n",
    "    --system_id 'vvercherg' \\\n",
    "    --runner DataflowRunner \\\n",
    "    --job_name 'data-flow-pruebas-1' \\\n",
    "    --region 'europe-west1' \\\n",
    "    --temp_location 'gs://dataflow_bucket_dataproject_2425/tmp' \\\n",
    "    --staging_location 'gs://dataflow_bucket_dataproject_2425/stg' \\\n",
    "    --requirements_file 'requirements.txt'\n",
    "\n",
    "    \n",
    "    de aqui sacamos en claro que lee los mensajes y que le hace la window de 90 segundos\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora vamos a hacer que haga los matches y que los envie a otro pubsub,vamos a ver que nos sale KAKAKAKKA\n",
    "\n",
    "estoy desvariando ya creo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.metrics import Metrics\n",
    "\n",
    "# B. Apache Beam ML Libraries\n",
    "from apache_beam.ml.inference.base import ModelHandler\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "\n",
    "# C. Python Libraries\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "\n",
    "beam.options.pipeline_options.PipelineOptions.allow_non_parallel_instruction_output = True\n",
    "DataflowRunner.__test__ = False\n",
    "\n",
    "def ParsePubSubMessages(message): \n",
    "    pubsub_message= message.decode('utf-8')\n",
    "\n",
    "    msg = json.loads(pubsub_message)\n",
    "\n",
    "    logging.info(\"New message: %s\", msg)\n",
    "\n",
    "    return msg['city'], msg\n",
    "\n",
    "class BusinessLogicDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "\n",
    "        city, grouped_data = element\n",
    "        affected_list = grouped_data.get('affected', [])\n",
    "        volunteer_list = grouped_data.get('volunteer', [])\n",
    "        \n",
    "        matched_data = []\n",
    "        unmatched_data = []\n",
    "        \n",
    "        for affected in affected_list:\n",
    "            matched = False\n",
    "            for volunteer in volunteer_list:\n",
    "                if (affected['necessity'] == volunteer['necessity'] and\n",
    "                    affected['disponibility'] == volunteer['disponibility'] and\n",
    "                    affected['city'] == volunteer['city']\n",
    "                    ):\n",
    "                    matched_data.append({\n",
    "                        \"affected\": affected,\n",
    "                        \"volunteer\": volunteer,\n",
    "                        \"matched_timestamp\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                    })\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                unmatched_data.append(affected)\n",
    "        \n",
    "        for volunteer in volunteer_list:\n",
    "            if all(volunteer != match['volunteer'] for match in matched_data):\n",
    "                unmatched_data.append(volunteer)\n",
    "        \n",
    "        yield beam.pvalue.TaggedOutput(\"matched_data\", matched_data)\n",
    "        yield beam.pvalue.TaggedOutput(\"unmatched_data\", unmatched_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run(): \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=('Input arguments for the Dataflow Streaming Pipeline.'))\n",
    "\n",
    "    parser.add_argument(\n",
    "                '--project_id',\n",
    "                required=True,\n",
    "                help='GCP cloud project name, in this case data-project-2425')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--affected_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading affected people. In this case the subscripcion will be: affected-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--volunteer_sub',\n",
    "                required=True,\n",
    "                help='PubSub sub used for reading volunteer prople. In this case the subscripcion will be: volunteer-sub')\n",
    "    \n",
    "    parser.add_argument(\n",
    "                '--output_topic_non_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of non matched messages. In this case it will be: no-matched')\n",
    "        \n",
    "    parser.add_argument(\n",
    "                '--output_topic_matched',\n",
    "                required=True,\n",
    "                help='PubSub Topic for storing data of matched messages. In this case: matched')\n",
    "    \n",
    "    args, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    options = PipelineOptions(pipeline_opts, \n",
    "        save_main_session= True, streaming= True, project= args.project_id)\n",
    "    \n",
    "    with beam.Pipeline(argv= pipeline_opts, options=options) as p:\n",
    "\n",
    "        affected_data=(\n",
    "            p\n",
    "                |\"Read affected data from Pub/Sub\" >> beam.io.ReadFromPubSub(subscription= args.affected_sub)\n",
    "                |\"Parse Json battery messages\" >> beam.Map(ParsePubSubMessages)\n",
    "                |\" Fixed window for Affected data\" >>beam.WindowInto(beam.window.FixedWindows(90))\n",
    "        )\n",
    "\n",
    "        volunteer_data=(\n",
    "            p \n",
    "                |\"Read volunteer data from Pub/Sub\">> beam.io.ReadFromPubSub(subscription=args.volunteer_sub)\n",
    "                |\"Parse Json from Volunteer messages\">> beam.Map(ParsePubSubMessages)\n",
    "                |\"Fixed window for Volunteer data\" >> beam.WindowInto(beam.window.FixedWindows(90))\n",
    "        )\n",
    "\n",
    "        # co Group by key \n",
    "        grouped_data= (\n",
    "            affected_data, volunteer_data) | \"merge PCollection\" >> beam.CoGroupByKey()\n",
    "\n",
    "        proces_data= (grouped_data\n",
    "            |\"check the matched messages\" >> beam.ParDo(BusinessLogicDoFn()).with_outputs(\"matched_data\", \"unmatched_data\"))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Disable logs from apache_beam.utils.subprocess_server\n",
    "    logging.getLogger(\"apache_beam.utils.subprocess_server\").setLevel(logging.ERROR)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "\n",
    "    # Run Process\n",
    "    run()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Till here i want to prove if the code is correct let's run it on dataflow  run pipeline in GCP: dataflow\n",
    "\n",
    "        python dataflow_pipeline.py \\\n",
    "    --project_id 'data-project-2425' \\\n",
    "    --affected_sub 'projects/data-project-2425/subscriptions/affected-sub' \\\n",
    "    --volunteer_sub 'projects/data-project-2425/subscriptions/volunteer-sub' \\\n",
    "    --output_topic_non_matched 'projects/data-project-2425/topics/no-matched' \\\n",
    "    --output_topic_matched 'projects/data-project-2425/topics/matched' \\\n",
    "    --system_id 'vvercherg' \\\n",
    "    --runner DataflowRunner \\\n",
    "    --job_name 'data-flow-pruebas-1' \\\n",
    "    --region 'europe-west1' \\\n",
    "    --temp_location 'gs://dataflow_bucket_dataproject_2425/tmp' \\\n",
    "    --staging_location 'gs://dataflow_bucket_dataproject_2425/stg' \\\n",
    "    --requirements_file 'requirements.txt'\n",
    "\n",
    "    \n",
    "    de aqui sacamos en claro que lee los mensajes y que le hace la window de 90 segundos\n",
    "        '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
